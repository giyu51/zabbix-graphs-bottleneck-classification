{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customization\n",
    "\n",
    "with open(\"zabbix_conf.json\", 'r') as conf_file:\n",
    "    conf_file = json.load(conf_file)\n",
    "    training_conf = conf_file['Model Training']\n",
    "    \n",
    "# Set the path to the main directory containing the training and testing folders\n",
    "main_dir =training_conf['main_dir']\n",
    "\n",
    "# Define the subdirectories\n",
    "train_dir =training_conf['train_dir']\n",
    "test_dir =training_conf['test_dir']\n",
    "logs_dir =training_conf['logs_dir']\n",
    "\n",
    "# Set the desired image settings\n",
    "image_width =training_conf['image_width']\n",
    "image_height =training_conf['image_height']\n",
    "image_size = (image_width, image_height)\n",
    "resize_factor =training_conf['resize_factor']\n",
    "batch_size =training_conf['batch_size']\n",
    "\n",
    "# Specify the exact names of the two classes (two directories) located in both subdirectories\n",
    "class_names =training_conf['class_names']\n",
    "epochs = training_conf['epochs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('_________________________________________________________')\n",
    "print(f'Main directory is set to {main_dir}')\n",
    "print(f'Training subdirectory is set to {train_dir}')\n",
    "print(f'Testing subdirectory is set to {test_dir}')\n",
    "print('_________________________________________________________')\n",
    "print(f'Image width: {image_width}')\n",
    "print(f'Image height: {image_height}')\n",
    "print(f'Image will be resized to {resize_factor*100}% of its original size')\n",
    "print('_________________________________________________________')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Number of training cycles (epochs): {epochs}')\n",
    "print('_________________________________________________________')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    class_names=class_names,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create the testing dataset\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    class_names=class_names,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply a function to the datasets using the map() method\n",
    "# Normalize pixel values to [0, 1]\n",
    "train_dataset = train_dataset.map(lambda x, y: (x / 255.0, y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (x / 255.0, y))\n",
    "\n",
    "\n",
    "reduced_image_size = (int(image_size[0] * resize_factor), int(image_size[1] * resize_factor))\n",
    "\n",
    "# Resize images in the datasets\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.image.resize(x, reduced_image_size), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (tf.image.resize(x, reduced_image_size), y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(reduced_image_size[0], reduced_image_size[1], 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "testing_loss, testing_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(f'Test Loss: {testing_loss}, Test Accuracy: {testing_accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the accuracy and loss values from the history object\n",
    "training_accuracy = history.history['accuracy']\n",
    "training_loss = history.history['loss']\n",
    "\n",
    "# Create subplots for accuracy and loss\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(range(1, len(training_accuracy) + 1), training_accuracy, label='Training Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Training Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(range(1, len(training_loss) + 1), training_loss, label='Training Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Training Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'{logs_dir}/training_plot.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {\n",
    "    \"Time\": {\n",
    "        \"date\":datetime.datetime.now().strftime('%D'),\n",
    "        \"time\":datetime.datetime.now().strftime('%T')\n",
    "    },\n",
    "    \n",
    "    \"DIR-Config\": {\n",
    "        \"Main DIR\": main_dir,\n",
    "        \"Training SUB-DIR\": train_dir,\n",
    "        \"Testing SUB-DIR\": test_dir\n",
    "    },\n",
    "    \n",
    "    \"Image-Config\": {\n",
    "        \"Image width\":image_width,\n",
    "        \"Image height\":image_height,\n",
    "        \"Image resize\":resize_factor\n",
    "    },\n",
    "    \n",
    "    \"Training Config\": {\n",
    "        \"Batch size\": batch_size,\n",
    "        \"Epochs\": epochs,\n",
    "    },\n",
    "    \n",
    "    \"MODEL OUT\": {\n",
    "      \"Training Loss\":training_loss[-1],\n",
    "      \"Training Accuracy\":training_accuracy[-1],\n",
    "      \"Training Progress\": [f\"Epoch {epoch+1}: Accuracy = {training_accuracy[epoch]}, Loss = {training_loss[epoch]}\" for epoch in range(len(training_accuracy))],\n",
    "\n",
    "      \"Testing Loss\":testing_loss,\n",
    "      \"Testing Accuracy\": testing_accuracy,\n",
    "\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# Write the data to json file\n",
    "with open(f\"{logs_dir}/zabbix_logs.json\", \"a\") as json_file:\n",
    "    json.dump(logs, json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./MK-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
